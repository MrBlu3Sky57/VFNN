{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "7c4eb13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ccxt\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "54806a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68769\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the exchange object (replace 'huobi' with your desired exchange)\n",
    "exchange = ccxt.binance()\n",
    "\n",
    "# Symbol to fetch market data\n",
    "symbol = 'BTC/USDT'\n",
    "timeframe = '1h'\n",
    "limit = 1000\n",
    "\n",
    "since = exchange.parse8601('2017-01-01T00:00:00Z')\n",
    "\n",
    "ohlvc = []\n",
    "while since < exchange.milliseconds():\n",
    "    data = exchange.fetch_ohlcv(symbol=symbol, timeframe=timeframe, since=since, limit=limit)\n",
    "    if not data:\n",
    "        break\n",
    "    ohlvc.extend(data)\n",
    "    since = data[-1][0] + 1 # Move forward in time\n",
    "    time.sleep(exchange.rateLimit / 1000)\n",
    "print(len(ohlvc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "ddde9d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate relevant statistics\n",
    "# Data is in form: open -- high -- low -- close -- volume\n",
    "raw_data = torch.tensor(ohlvc)[:, 1:]\n",
    "open, high, low, close, volume = raw_data[:, 0], raw_data[:, 1], raw_data[:, 2], raw_data[:, 3], raw_data[:, 4]\n",
    "spread = high - low\n",
    "diff = close - open\n",
    "body = torch.abs(diff) / (spread + 1e-12)\n",
    "log_return = torch.log(close[1:] / close[:-1]) # Different shape --> Discard first elem\n",
    "log_vol = torch.log(volume + 1e-12)\n",
    "vol_change = torch.log((volume[1:] + 1e-12) / (volume[:-1] + 1e-12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "98d1db9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "def window(data: torch.Tensor, op: callable, size: int = N):\n",
    "    out = []\n",
    "    for i in range(size, data.shape[0]):\n",
    "        out.append(op(data[i - size:i]))\n",
    "    return torch.stack(out)\n",
    "\n",
    "close_mean = window(close, torch.mean)\n",
    "close_std = window(close, torch.std)\n",
    "vol_mean = window(volume, torch.mean)\n",
    "momentum = close[N:] / close[:-N]\n",
    "BB_width = 4 * close_std / close_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feda61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([68745, 15, 11])\n",
      "torch.Size([68745])\n"
     ]
    }
   ],
   "source": [
    "# Trim features\n",
    "acc_spread = spread[N-1:-1]\n",
    "acc_diff = diff[N-1:-1]\n",
    "acc_body = body[N-1:-1]\n",
    "acc_log_return = log_return[N-2:-1]\n",
    "acc_log_vol = log_vol[N-1:-1]\n",
    "acc_vol_change = vol_change[N-2:-1]\n",
    "\n",
    "# Put into feature list\n",
    "features = [acc_spread, acc_diff, acc_body, acc_log_return, acc_log_vol, acc_vol_change, close_mean, close_std, vol_mean, momentum, BB_width]\n",
    "\n",
    "# Build input tensor\n",
    "d = 15 # Sequence length\n",
    "xs = torch.stack([f.unfold(0, d, 1) for f in features], dim=2)\n",
    "print(xs.shape)\n",
    "\n",
    "# Targets\n",
    "targets = log_return[N + d - 2:] ** 2\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc90e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gate(nn.Module):\n",
    "    def __init__(self, in_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.in_weight = nn.Parameter(torch.randn(shape=(in_size, hidden_size), requires_grad=True))\n",
    "        self.hidden_weight = nn.Parameter(torch.randn(shape=(hidden_size, hidden_size), requires_grad=True))\n",
    "        self.bias = nn.Parameter(torch.randn(shape=(hidden_size), requires_grad=True))\n",
    "        self.act = torch.sigmoid\n",
    "        \n",
    "    def forward(self, x_inp, hidden_inp):\n",
    "        unact = hidden_inp @ self.hidden_weight + x_inp @ self.in_weight + self.bias\n",
    "        return self.act(unact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7fbb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Cell(nn.Module):\n",
    "    def __init__(self, in_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.forget_gate = Gate(in_size, hidden_size)\n",
    "        self.input_gate = Gate(in_size, hidden_size)\n",
    "        self.out_gate = Gate(in_size, hidden_size)\n",
    "        self.inp_weight = nn.Parameter(torch.randn(shape=(in_size, hidden_size), requires_grad=True))\n",
    "        self.hidden_weight = nn.Parameter(torch.randn(shape=(hidden_size, hidden_size), requires_grad=True))\n",
    "        self.bias = nn.Parameter(torch.randn(shape=(hidden_size), requires_grad=True))\n",
    "    \n",
    "    def forward(self, x_inp, hidden_inp, memory_inp):\n",
    "        forget_gate = self.forget_gate(x_inp, hidden_inp)\n",
    "        input_gate = self.input_gate(x_inp, hidden_inp)\n",
    "        out_gate = self.out_gate(x_inp, hidden_inp)\n",
    "\n",
    "        candidate_mem = torch.tanh(hidden_inp @ self.hidden_weight + x_inp @ self.inp_weight + self.bias)\n",
    "        new_mem = forget_gate * memory_inp + input_gate * candidate_mem\n",
    "        new_output = out_gate * torch.tanh(new_mem)\n",
    "        return (new_output, new_mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b03435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature dim is always last dim\n",
    "class BatchNorm1D(nn.Module):\n",
    "    def __init__(self, num_features, training=True, momentum=0.9):\n",
    "        super().__init__()\n",
    "        self.running_mu = torch.ones((num_features))\n",
    "        self.running_var = torch.zeros((num_features))\n",
    "        self.gamma = nn.Parameter(torch.ones((num_features), requires_grad=True))\n",
    "        self.beta = nn.Parameter(torch.zeros((num_features), requires_grad=True))\n",
    "        self.training = training\n",
    "        self.momentum = momentum\n",
    "    \n",
    "    def forward(self, inp: torch.Tensor):\n",
    "        ndims = inp.ndim - 1\n",
    "        c = self.running_mu.shape[0]\n",
    "        if self.training:\n",
    "            mu = torch.mean(inp, dim=(inp.shape[:-1]), keepdim=True)\n",
    "            var = torch.std(inp, dim=(inp.shape[:-1]), keepdim=True)\n",
    "        else:\n",
    "            mu = self.running_mu.view(*([1] * ndims), c)\n",
    "            mu = self.running_var.view(*([1] * ndims), c)\n",
    "        \n",
    "        gamma = self.gamma.view(*([1] * ndims), c)\n",
    "        beta = self.beta.view(*([1] * ndims), c)\n",
    "\n",
    "        # Normalize then reparametrize\n",
    "        x_hat = (inp - mu) / var\n",
    "        x_new = gamma * x_hat + beta\n",
    "\n",
    "        # Update running average -- if training\n",
    "        if self.training:\n",
    "            self.running_mu = momentum * self.running_mu + (1 - momentum) * mu\n",
    "            self.running_var = momentum * self.running_var + (1 - momentum) * var\n",
    "        \n",
    "        return x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311e563f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, in_size, hidden_size, training=True):\n",
    "        super().__init__()\n",
    "        self.cell = Cell(in_size, hidden_size)\n",
    "        if training: # Batch mode\n",
    "            self.initial_hidden = torch.zeros((1, hidden_size))\n",
    "            self.initial_mem = torch.zeros((1, hidden_size))\n",
    "        else:\n",
    "            self.initial_hidden = torch.zeros(hidden_size)\n",
    "            self.initial_mem = torch.zeros(hidden_size)\n",
    "        \n",
    "        self.in_batchnorm = BatchNorm1D(in_size)\n",
    "        self.hidden_batchnorm = BatchNorm1D(hidden_size)\n",
    "        self.training = training\n",
    "    \n",
    "    def forward(self, inp: torch.Tensor):\n",
    "        hidden = self.initial_hidden\n",
    "        mem = self.initial_mem\n",
    "        if inp.ndim == 2:\n",
    "            channels = inp.shape[0]\n",
    "        elif inp.ndim == 3:\n",
    "            channels = inp.shape[1]\n",
    "        else:\n",
    "            raise ValueError\n",
    "        \n",
    "        for t in range(channels):\n",
    "            \n",
    "            # Ensure no batchnorm bugs\n",
    "            if self.training != self.in_batchnorm.training:\n",
    "                self.in_batchnorm.training = self.training\n",
    "            if self.training != self.hidden_batchnorm.training:\n",
    "                self.hidden_batchnorm.training = self.training\n",
    "\n",
    "            # Deal with dimension cases\n",
    "            if inp.ndim == 2:\n",
    "                x = self.in_batchnorm(inp[t, :])\n",
    "            else:\n",
    "                x = self.in_batchnorm(inp[:, t, :])\n",
    "\n",
    "            # Cell update\n",
    "            hidden = self.hidden_batchnorm(hidden)\n",
    "            hidden, mem = self.cell(x, hidden, mem)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d50f811",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, size: tuple, training=True):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential()\n",
    "        for l1, l2 in zip(size, size[1:]):\n",
    "            self.layers.append(nn.Linear(l1, l2))\n",
    "            self.layers.append(nn.Tanh())\n",
    "        self.layers.pop()\n",
    "        self.training=training\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        return self.layers.forward(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae43985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(logits, targets) -> torch.Tensor:\n",
    "    return 0.5 * torch.log(logits * 2 * torch.pi) + (targets ** 2) / (2 * logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c845091a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VFNN(nn.Module):\n",
    "    def __init__(self, hidden_size: tuple, inp_size: tuple, MLP_size: tuple, training=True):\n",
    "        super().__init__()\n",
    "        assert MLP_size[0] == hidden_size\n",
    "        self.blocks = nn.Sequential\n",
    "        self.blocks.append(LSTM(in_size=inp_size, hidden_size=hidden_size))\n",
    "        self.blocks.append(MLP(MLP_size))\n",
    "        self.training = training\n",
    "    \n",
    "    def forward(self, inp, targets=None):\n",
    "        sigma = self.blocks.forward(inp)\n",
    "        if self.training:\n",
    "            loss = nll(sigma, targets).mean()\n",
    "            return sigma, loss\n",
    "        else:\n",
    "            return sigma\n",
    "    \n",
    "    def set_predict(self):\n",
    "        self.blocks[0].training = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
