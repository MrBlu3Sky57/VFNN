{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "7c4eb13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ccxt\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "54806a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68769\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the exchange object (replace 'huobi' with your desired exchange)\n",
    "exchange = ccxt.binance()\n",
    "\n",
    "# Symbol to fetch market data\n",
    "symbol = 'BTC/USDT'\n",
    "timeframe = '1h'\n",
    "limit = 1000\n",
    "\n",
    "since = exchange.parse8601('2017-01-01T00:00:00Z')\n",
    "\n",
    "ohlvc = []\n",
    "while since < exchange.milliseconds():\n",
    "    data = exchange.fetch_ohlcv(symbol=symbol, timeframe=timeframe, since=since, limit=limit)\n",
    "    if not data:\n",
    "        break\n",
    "    ohlvc.extend(data)\n",
    "    since = data[-1][0] + 1 # Move forward in time\n",
    "    time.sleep(exchange.rateLimit / 1000)\n",
    "print(len(ohlvc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "ddde9d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate relevant statistics\n",
    "# Data is in form: open -- high -- low -- close -- volume\n",
    "raw_data = torch.tensor(ohlvc)[:, 1:]\n",
    "open, high, low, close, volume = raw_data[:, 0], raw_data[:, 1], raw_data[:, 2], raw_data[:, 3], raw_data[:, 4]\n",
    "spread = high - low\n",
    "diff = close - open\n",
    "body = torch.abs(diff) / (spread + 1e-12)\n",
    "log_return = torch.log(close[1:] / close[:-1]) # Different shape --> Discard first elem\n",
    "log_vol = torch.log(volume + 1e-12)\n",
    "vol_change = torch.log((volume[1:] + 1e-12) / (volume[:-1] + 1e-12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "98d1db9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "def window(data: torch.Tensor, op: callable, size: int = N):\n",
    "    out = []\n",
    "    for i in range(size, data.shape[0]):\n",
    "        out.append(op(data[i - size:i]))\n",
    "    return torch.stack(out)\n",
    "\n",
    "close_mean = window(close, torch.mean)[:-1]\n",
    "close_std = window(close, torch.std)[:-1]\n",
    "vol_mean = window(volume, torch.mean)[:-1]\n",
    "momentum = (close[N:] / close[:-N])[:-1]\n",
    "BB_width =(4 * close_std / close_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "3feda61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([68744, 15, 11])\n",
      "torch.Size([68744])\n"
     ]
    }
   ],
   "source": [
    "# Trim features\n",
    "acc_spread = spread[N:-1]\n",
    "acc_diff = diff[N:-1]\n",
    "acc_body = body[N:-1]\n",
    "acc_log_return = log_return[N-1:-1]\n",
    "acc_log_vol = log_vol[N:-1]\n",
    "acc_vol_change = vol_change[N-1:-1]\n",
    "\n",
    "# Put into feature list\n",
    "features = [acc_spread, acc_diff, acc_body, acc_log_return, acc_log_vol, acc_vol_change, close_mean, close_std, vol_mean, momentum, BB_width]\n",
    "\n",
    "# Build input tensor\n",
    "d = 15 # Sequence length\n",
    "xs = torch.stack([f.unfold(0, d, 1) for f in features], dim=2)\n",
    "print(xs.shape)\n",
    "\n",
    "# Targets\n",
    "targets = log_return[N + d - 1:] ** 2\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d44d31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "num_samples = xs.shape[0]\n",
    "\n",
    "# 70/15/15 split (or 80/10/10)\n",
    "train_end = int(0.7 * num_samples)\n",
    "val_end = int(0.85 * num_samples)\n",
    "\n",
    "# DO NOT SHUFFLE YET\n",
    "xs_train = xs[:train_end]\n",
    "ys_train = targets[:train_end]\n",
    "\n",
    "xs_val = xs[train_end:val_end]\n",
    "ys_val = targets[train_end:val_end]\n",
    "\n",
    "xs_test = xs[val_end:]\n",
    "ys_test = targets[val_end:]\n",
    "\n",
    "perm = torch.randperm(xs_train.shape[0])\n",
    "xs_train = xs_train[perm]\n",
    "ys_train = ys_train[perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "fcc90e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gate(nn.Module):\n",
    "    def __init__(self, in_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.in_weight = nn.Parameter(torch.randn(shape=(in_size, hidden_size), requires_grad=True))\n",
    "        self.hidden_weight = nn.Parameter(torch.randn(shape=(hidden_size, hidden_size), requires_grad=True))\n",
    "        self.bias = nn.Parameter(torch.randn(shape=(hidden_size), requires_grad=True))\n",
    "        self.act = torch.sigmoid\n",
    "        \n",
    "    def forward(self, x_inp, hidden_inp):\n",
    "        unact = hidden_inp @ self.hidden_weight + x_inp @ self.in_weight + self.bias\n",
    "        return self.act(unact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "be7fbb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Cell(nn.Module):\n",
    "    def __init__(self, in_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.forget_gate = Gate(in_size, hidden_size)\n",
    "        self.input_gate = Gate(in_size, hidden_size)\n",
    "        self.out_gate = Gate(in_size, hidden_size)\n",
    "        self.inp_weight = nn.Parameter(torch.randn(shape=(in_size, hidden_size), requires_grad=True))\n",
    "        self.hidden_weight = nn.Parameter(torch.randn(shape=(hidden_size, hidden_size), requires_grad=True))\n",
    "        self.bias = nn.Parameter(torch.randn(shape=(hidden_size), requires_grad=True))\n",
    "    \n",
    "    def forward(self, x_inp, hidden_inp, memory_inp):\n",
    "        forget_gate = self.forget_gate(x_inp, hidden_inp)\n",
    "        input_gate = self.input_gate(x_inp, hidden_inp)\n",
    "        out_gate = self.out_gate(x_inp, hidden_inp)\n",
    "\n",
    "        candidate_mem = torch.tanh(hidden_inp @ self.hidden_weight + x_inp @ self.inp_weight + self.bias)\n",
    "        new_mem = forget_gate * memory_inp + input_gate * candidate_mem\n",
    "        new_output = out_gate * torch.tanh(new_mem)\n",
    "        return (new_output, new_mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "b8b03435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature dim is always last dim\n",
    "class BatchNorm1D(nn.Module):\n",
    "    def __init__(self, num_features, training=True, momentum=0.9):\n",
    "        super().__init__()\n",
    "        self.running_mu = torch.ones((num_features))\n",
    "        self.running_var = torch.zeros((num_features))\n",
    "        self.gamma = nn.Parameter(torch.ones((num_features), requires_grad=True))\n",
    "        self.beta = nn.Parameter(torch.zeros((num_features), requires_grad=True))\n",
    "        self.training = training\n",
    "        self.momentum = momentum\n",
    "    \n",
    "    def forward(self, inp: torch.Tensor):\n",
    "        ndims = inp.ndim - 1\n",
    "        c = self.running_mu.shape[0]\n",
    "        if self.training:\n",
    "            mu = torch.mean(inp, dim=(inp.shape[:-1]), keepdim=True)\n",
    "            var = torch.std(inp, dim=(inp.shape[:-1]), keepdim=True)\n",
    "        else:\n",
    "            mu = self.running_mu.view(*([1] * ndims), c)\n",
    "            mu = self.running_var.view(*([1] * ndims), c)\n",
    "        \n",
    "        gamma = self.gamma.view(*([1] * ndims), c)\n",
    "        beta = self.beta.view(*([1] * ndims), c)\n",
    "\n",
    "        # Normalize then reparametrize\n",
    "        x_hat = (inp - mu) / var\n",
    "        x_new = gamma * x_hat + beta\n",
    "\n",
    "        # Update running average -- if training\n",
    "        if self.training:\n",
    "            self.running_mu = momentum * self.running_mu + (1 - momentum) * mu\n",
    "            self.running_var = momentum * self.running_var + (1 - momentum) * var\n",
    "        \n",
    "        return x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "311e563f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, in_size, hidden_size, training=True):\n",
    "        super().__init__()\n",
    "        self.cell = Cell(in_size, hidden_size)\n",
    "        if training: # Batch mode\n",
    "            self.initial_hidden = torch.zeros((1, hidden_size))\n",
    "            self.initial_mem = torch.zeros((1, hidden_size))\n",
    "        else:\n",
    "            self.initial_hidden = torch.zeros(hidden_size)\n",
    "            self.initial_mem = torch.zeros(hidden_size)\n",
    "        \n",
    "        self.in_batchnorm = BatchNorm1D(in_size)\n",
    "        self.hidden_batchnorm = BatchNorm1D(hidden_size)\n",
    "        self.training = training\n",
    "    \n",
    "    def forward(self, inp: torch.Tensor):\n",
    "        hidden = self.initial_hidden\n",
    "        mem = self.initial_mem\n",
    "        if inp.ndim == 2:\n",
    "            channels = inp.shape[0]\n",
    "        elif inp.ndim == 3:\n",
    "            channels = inp.shape[1]\n",
    "        else:\n",
    "            raise ValueError\n",
    "        \n",
    "        for t in range(channels):\n",
    "            \n",
    "            # Ensure no batchnorm bugs\n",
    "            if self.training != self.in_batchnorm.training:\n",
    "                self.in_batchnorm.training = self.training\n",
    "            if self.training != self.hidden_batchnorm.training:\n",
    "                self.hidden_batchnorm.training = self.training\n",
    "\n",
    "            # Deal with dimension cases\n",
    "            if inp.ndim == 2:\n",
    "                x = self.in_batchnorm(inp[t, :])\n",
    "            else:\n",
    "                x = self.in_batchnorm(inp[:, t, :])\n",
    "\n",
    "            # Cell update\n",
    "            hidden = self.hidden_batchnorm(hidden)\n",
    "            hidden, mem = self.cell(x, hidden, mem)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "3d50f811",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, size: tuple, training=True):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential()\n",
    "        for l1, l2 in zip(size, size[1:]):\n",
    "            self.layers.append(nn.Linear(l1, l2))\n",
    "            self.layers.append(nn.Tanh())\n",
    "        self.layers.pop()\n",
    "        self.training=training\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        return self.layers.forward(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae43985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(logits, targets) -> torch.Tensor:\n",
    "    return 0.5 * torch.log(logits * 2 * torch.pi) + (targets ** 2) / (2 * logits)\n",
    "\n",
    "def clip_grad(tensor: torch.Tensor, norm = 5.0):\n",
    "    if torch.norm(tensor.grad).item() >= norm:\n",
    "        tensor.grad /= norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c845091a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VFNN(nn.Module):\n",
    "    def __init__(self, hidden_size: int, inp_size: int, MLP_size: tuple, training=True):\n",
    "        super().__init__()\n",
    "        assert MLP_size[0] == hidden_size\n",
    "        self.blocks = nn.Sequential\n",
    "        self.blocks.append(LSTM(in_size=inp_size, hidden_size=hidden_size))\n",
    "        self.blocks.append(MLP(MLP_size))\n",
    "        self.training = training\n",
    "    \n",
    "    def forward(self, inp) -> torch.Tensor:\n",
    "        sigma = self.blocks.forward(inp)\n",
    "        return sigma\n",
    "    \n",
    "    def set_predict(self):\n",
    "        self.blocks[0].training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b40b41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(model: VFNN, xs: torch.Tensor, ys: torch.Tensor, lr: float = 0.1, batch_size: int = 30, steps: int = 1000):\n",
    "    \"\"\"\n",
    "    Perform stochastic gradient descent on model\n",
    "    \"\"\"\n",
    "    lossi = []\n",
    "    for step in range(steps):\n",
    "        idx = torch.randint(0, len(xs), (batch_size))\n",
    "        x_batch, y_batch = xs[idx], ys[idx]\n",
    "\n",
    "        sigma = model.forward(x_batch, y_batch)\n",
    "        loss = nll(sigma, y_batch).nean()\n",
    "\n",
    "        l2_lambda = torch.tensor(1e-4)\n",
    "        l2_penalty = torch.tensor(0)\n",
    "        for param in model.parameters():\n",
    "            param.grad = None\n",
    "            l2_penalty += (param * param).sum()\n",
    "\n",
    "        loss = loss + l2_lambda * l2_penalty\n",
    "        loss.backward()\n",
    "        lossi.append(loss.log10().item())\n",
    "        \n",
    "\n",
    "        for param in model.parameters():\n",
    "            clip_grad(param)\n",
    "            param += -lr * param.grad\n",
    "    \n",
    "        if step == steps // 2:\n",
    "            lr = lr * 0.1\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Loss: {loss.item() / 100} on step: {step + 1}\")\n",
    "    return lossi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f86dbb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "randn() received an invalid combination of arguments - got (requires_grad=bool, shape=tuple, ), but expected one of:\n * (tuple of ints size, *, torch.Generator generator, tuple of names names, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, *, torch.Generator generator, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[244]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mVFNN\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m11\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m lossi = sgd(model, xs_train, ys_train, batch_size=\u001b[32m60\u001b[39m, steps=\u001b[32m1000\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[242]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mVFNN.__init__\u001b[39m\u001b[34m(self, hidden_size, inp_size, MLP_size, training)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m MLP_size[\u001b[32m0\u001b[39m] == hidden_size\n\u001b[32m      5\u001b[39m \u001b[38;5;28mself\u001b[39m.blocks = nn.Sequential\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28mself\u001b[39m.blocks.append(\u001b[43mLSTM\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43minp_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mself\u001b[39m.blocks.append(MLP(MLP_size))\n\u001b[32m      8\u001b[39m \u001b[38;5;28mself\u001b[39m.training = training\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[237]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mLSTM.__init__\u001b[39m\u001b[34m(self, in_size, hidden_size, training)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, in_size, hidden_size, training=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m      3\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[38;5;28mself\u001b[39m.cell = \u001b[43mCell\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m training: \u001b[38;5;66;03m# Batch mode\u001b[39;00m\n\u001b[32m      6\u001b[39m         \u001b[38;5;28mself\u001b[39m.initial_hidden = torch.zeros((\u001b[32m1\u001b[39m, hidden_size))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[235]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mCell.__init__\u001b[39m\u001b[34m(self, in_size, hidden_size)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, in_size, hidden_size):\n\u001b[32m      3\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[38;5;28mself\u001b[39m.forget_gate = \u001b[43mGate\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mself\u001b[39m.input_gate = Gate(in_size, hidden_size)\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mself\u001b[39m.out_gate = Gate(in_size, hidden_size)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[234]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mGate.__init__\u001b[39m\u001b[34m(self, in_size, hidden_size)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, in_size, hidden_size):\n\u001b[32m      3\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[38;5;28mself\u001b[39m.in_weight = nn.Parameter(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mself\u001b[39m.hidden_weight = nn.Parameter(torch.randn(shape=(hidden_size, hidden_size), requires_grad=\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mself\u001b[39m.bias = nn.Parameter(torch.randn(shape=(hidden_size), requires_grad=\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "\u001b[31mTypeError\u001b[39m: randn() received an invalid combination of arguments - got (requires_grad=bool, shape=tuple, ), but expected one of:\n * (tuple of ints size, *, torch.Generator generator, tuple of names names, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, *, torch.Generator generator, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n"
     ]
    }
   ],
   "source": [
    "model = VFNN(128, 11, (128, 64, 64, 1))\n",
    "lossi = sgd(model, xs_train, ys_train, batch_size=60, steps=1000)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(torch.tensor(lossi).view(-1, 100).mean(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
